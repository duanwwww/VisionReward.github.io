<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="description" content="Fine-Grained Multi-Dimensional Human Preference Learning
for Image and Video Generation" />
    <meta name="keywords" content="TODO" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
        VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning
        for Image and Video Generation
    </title>
    <!-- Bulma CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css" rel="stylesheet">

    <!-- FontAwesome Icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet">
    <style>
        /* ÈÄöÁî®Â±Ö‰∏≠Ê†∑Âºè */
        body {
            margin: 0;
            font-family: Arial, sans-serif;
            text-align: center;
        }

        header {
            margin: 20px 0;
        }

        h1 {
            font-size: 2em;
            margin: 10px 0;
        }

        h2 {
            margin: 40px 0 10px;
        }

        a {
            text-decoration: none;
            color: blue;
        }

        a:hover {
            text-decoration: underline;
        }

        table {
            width: 80%;
            margin: 20px auto;
            border-collapse: collapse;
            font-size: 16px;
            text-align: center;
            /* table-layout: fixed; */
        }

        th,
        td {
            padding: 10px;
            vertical-align: middle;
            /* ÂûÇÁõ¥Â±Ö‰∏≠ */
        }

        th {
            font-weight: bold;
            background-color: #f4f4f4;
        }

        tr:nth-child(even) {
            background-color: #f9f9f9;
        }

        tr:nth-child(odd) {
            background-color: #ffffff;
        }

        tr.image-row {
            background-color: #ffffff;
            /* ÊâÄÊúâ Image Ë°åÁôΩËâ≤ËÉåÊôØ */
        }

        tr.video-row {
            background-color: #f0f0f0;
            /* ÊâÄÊúâ Video Ë°åÁÅ∞Ëâ≤ËÉåÊôØ */
        }

        table,
        th,
        td {
            border: none;
            /* ÁßªÈô§ËæπÊ°Ü */
        }

        /* ÂØºËà™Ê†èÊ†∑Âºè */
        .navbar {
            display: flex;
            justify-content: center;
            background-color: #333;
            padding: 10px 0;
            position: sticky;
            top: 0;
            z-index: 1000;
        }

        .navbar a {
            color: white;
            padding: 10px 20px;
            text-decoration: none;
        }

        .navbar a:hover {
            background-color: #575757;
            border-radius: 5px;
        }

        /* È°µÈù¢ÂÜÖÂÆπÂàÜÂå∫Ê†∑Âºè */
        section {
            padding: 20px;
            margin: 20px 0;
            border: 1px solid #ddd;
            border-radius: 5px;
            width: 80%;
            margin-left: auto;
            margin-right: auto;
        }
    </style>
</head>

<body>
    <header style="margin-top: 100px;">
        <h1>VisionReward</h1>
        <p style="font-size: 24px;font-family: 'Times New Roman', Times, serif;">
            Fine-Grained Multi-Dimensional Human Preference Learning for Image and
            Video Generation
        </p>
    </header>
    <div>
        <span>Vision Reward Team</span>
    </div>
    <div>
        <div>
            <span>
                <a href="TODO" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>arXiv</span>
                </a>
            </span>
            <span>
                <a href="https://github.com/xujz18/VisionReward"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                </a>
            </span>
        </div>
    </div>

    <div style="display: flex; justify-content: center;">
        <figure style="max-width: 60%; height: auto;">
            <img src="https://duanwwww.github.io/VisionReward.github.io/images/OverView.jpg"
                alt="An overview of the VisionReward and MPO." style="max-width: 100%; height: auto;" />
            <figcaption>An overview of the VisionReward and MPO.</figcaption>
        </figure>
    </div>


    <!-- Ê®™ÂêëÂØºËà™Ê†è -->
    <nav class="navbar">
        <a href="#overview">Overview</a>
        <a href="#intro">Introduction</a>
        <!-- <a href="annotation">Annotation</a> -->
        <!-- <a href="#benchmark">Benchmark</a> -->
        <a href="#experiments">Experiments</a>
        <a href="#case_study">Case Study</a>
        <a href="#citation">Citation</a>
    </nav>



    <!-- È°µÈù¢ÂÜÖÂÆπ -->
    <section id="overview">
        <h2 style="font-size: 2em;">Overview</h2>
        <div style="display: flex; justify-content: center;">
            <p style="max-width: 80%;text-align: left; font-size: 1em;">
                We present VisionReward, a general strategy to aligning visual generation
                models---both image and video generation---with human preferences through a
                fine-grained and multi-dimensional framework. We decompose human
                preferences in images and videos into multiple dimensions, each
                represented by a series of judgment questions, linearly weighted and
                summed to an interpretable and accurate score. To address the challenges
                of video quality assessment, we systematically analyze various dynamic
                features of videos, which helps VisionReward surpass VideoScore by 17.2%
                and achieve top performance for video preference prediction. Based on
                VisionReward, we develop a multi-objective preference learning algorithm
                that effectively addresses the issue of confounding factors within
                preference data. Our approach significantly outperforms existing image
                and video scoring methods on both machine metrics and human evaluation.
                The models and preference data will be open-sourced.
            </p>
        </div>

        <div style="display: flex; justify-content: center;">
            <figure style="max-width: 100%; height: auto;">
                <img src="https://duanwwww.github.io/VisionReward.github.io/images/TopDemo.jpg"
                    alt="Samples of VisionReward and Multi-Objective Preference Optimization (MPO) algorithm."
                    style="max-width: 100%; height: auto;" />
            </figure>
        </div>
        <div style="display: flex; justify-content: center; max-width: 100%;">
            <figure style="max-width: 33%; height: auto;">
                <video src="https://duanwwww.github.io/VisionReward.github.io/videos/ori_2.mp4"
                    style="width: 90%; height: auto;" controls>
                </video>
                <figcaption>original
                </figcaption>
            </figure>
            <figure style="max-width: 33%; height: auto;">
                <video src="https://duanwwww.github.io/VisionReward.github.io/videos/videoscore_2.mp4"
                    style="width: 90%; height: auto;" controls>
                </video>
                <figcaption>DPO with VideoScore
                </figcaption>
            </figure>
            <figure style="max-width: 33%; height: auto;">
                <video src="https://duanwwww.github.io/VisionReward.github.io/videos/ours_2.mp4"
                    style="width: 90%; height: auto;" controls>
                </video>
                <figcaption>MPO with VisionReward(Ours)
                </figcaption>
            </figure>
        </div>
        <p, style="text-align: center;">Samples of VisionReward and Multi-Objective Preference Optimization (MPO)
            algorithm.</p>
    </section>

    <section id="intro">
        <h2 style="font-size: 2em;">Introduction</h2>
        <div style="display: flex; flex-wrap: wrap; gap: 1rem; justify-content: center;">
            <!-- Âç°Áâá 1 -->
            <details
                style="width: 100%; border: 1px solid #ddd; border-radius: 8px; box-shadow: 0 2px 6px rgba(0,0,0,0.1); overflow: hidden;">
                <summary
                    style="cursor: pointer; padding: 1rem; background-color: #f9f9f9; font-weight: bold; font-size: 1.5em; border-bottom: 1px solid #ddd;">
                    Annotation
                </summary>
                <div style="padding: 1rem;display: flex;flex-wrap: wrap; justify-content: center;">
                    <p style="max-width: 80%;text-align: left; font-size: 1em;">üî∑ We design a unified annotation system
                        for both image and video
                        generation, decomposing the factors
                        influencing human preferences. To address the challenges of video evaluation, we incorporate
                        extensive
                        observations of dynamic content in videos into our judgment tasks, such as motion stability or
                        movement
                        quality. The annotation contains 3 million questions for 48k images and 2 million questions for
                        33k videos.
                    </p>
                    <table>
                        <thead>
                            <tr>
                                <th><strong>Type</strong></th>
                                <th><strong>Source</strong></th>
                                <th><strong>#Samples</strong></th>
                                <th><strong>#Checklist</strong></th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="image-row">
                                <td rowspan="3" style="vertical-align: middle; "><strong>Image</strong></td>
                                <td>ImageRewardDB</td>
                                <td>16K</td>
                                <td>1M</td>
                            </tr>
                            <tr class="image-row">
                                <td>Pick-a-Pic</td>
                                <td>16K</td>
                                <td>1M</td>
                            </tr>
                            <tr class="image-row">
                                <td>HPDv2</td>
                                <td>16K</td>
                                <td>1M</td>
                            </tr>
                            <tr class="video-row">
                                <td rowspan="4" style="vertical-align: middle; "><strong>Video</strong></td>
                                <td>CogVideoX</td>
                                <td>10K</td>
                                <td>0.6M</td>
                            </tr>
                            <tr class="video-row">
                                <td>Open-Sora</td>
                                <td>10K</td>
                                <td>0.6M</td>
                            </tr>
                            <tr class="video-row">
                                <td>VideoCrafter2</td>
                                <td>10K</td>
                                <td>0.6M</td>
                            </tr>
                            <tr class="video-row">
                                <td>Panda-70M</td>
                                <td>3K</td>
                                <td>0.2M</td>
                            </tr>
                        </tbody>
                    </table>
                    <p><em>Statistics of source data and annotation.</em></p>
                    </br>
                    <p style="max-width: 80%;text-align: left; font-size: 1em;">üî∑ When evaluating an image or video, human
                        preferences are often a result
                        of the interplay of multiple factors, necessitating a balance among various considerations. To
                        deconstruct human preferences systematically, we develop a fine-grained and multi-dimensional
                        preference decomposition framework.
                    </p>
                    <table>
                        <thead>
                            <tr>
                                <th rowspan="2" style="vertical-align: middle; "><strong>Dimension</strong></th>
                                <th colspan="2"><strong>#Sub-dimension</strong></th>
                                <th colspan="2"><strong>#Checklist</strong></th>
                            </tr>
                            <tr>
                                <th><strong>Image</strong></th>
                                <th><strong>Video</strong></th>
                                <th><strong>Image</strong></th>
                                <th><strong>Video</strong></th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Alignment</td>
                                <td>1</td>
                                <td>1</td>
                                <td>1</td>
                                <td>4</td>
                            </tr>
                            <tr>
                                <td>Composition</td>
                                <td>5</td>
                                <td>1</td>
                                <td>13</td>
                                <td>2</td>
                            </tr>
                            <tr>
                                <td>Quality</td>
                                <td>5</td>
                                <td>4</td>
                                <td>14</td>
                                <td>14</td>
                            </tr>
                            <tr>
                                <td>Fidelity</td>
                                <td>5</td>
                                <td>3</td>
                                <td>25</td>
                                <td>9</td>
                            </tr>
                            <tr>
                                <td>Safety & Emotion</td>
                                <td>2</td>
                                <td>1</td>
                                <td>8</td>
                                <td>4</td>
                            </tr>
                            <tr>
                                <td>Stability</td>
                                <td>-</td>
                                <td>5</td>
                                <td>-</td>
                                <td>12</td>
                            </tr>
                            <tr>
                                <td>Dynamic</td>
                                <td>-</td>
                                <td>2</td>
                                <td>-</td>
                                <td>8</td>
                            </tr>
                            <tr>
                                <td>Physics</td>
                                <td>-</td>
                                <td>1</td>
                                <td>-</td>
                                <td>4</td>
                            </tr>
                            <tr>
                                <td>Preservation</td>
                                <td>-</td>
                                <td>2</td>
                                <td>-</td>
                                <td>7</td>
                            </tr>
                            <tr>
                                <td><strong>Total</strong></td>
                                <td><strong>18</strong></td>
                                <td><strong>20</strong></td>
                                <td><strong>61</strong></td>
                                <td><strong>64</strong></td>
                            </tr>
                        </tbody>
                    </table>
                    <p><em>Taxonomy of annotation for VisionReward (Ours).</em></p>
                    </br>
                    <p style="max-width: 80%;text-align: left">üî∑ In our annotation process, we design multiple-choice
                        questions for each
                        dimension, allowing annotators to select the option that best fits the image. Additionally, we
                        further decompose these options into several binary (yes/no) questions, ultimately forming a
                        checklist for evaluating each image or video. This checklist is then utilized for subsequent
                        evaluation systems and reward model training.</p>
                    </br>
                    <p style="font-size: 1.2em; width: 100%;">Examples</p>
                    </br>
                    <table style="width: 90%;">
                        <!-- Ë°®Â§¥ -->
                        <thead>
                            <tr>
                                <th colspan="2" style="border-right: 1px">Color Aesthetic (Image)</th>
                                <th colspan="2">Movement Reality (Video)</th>
                            </tr>
                        </thead>
                        <!-- Ë°®Ê†ºÂÜÖÂÆπ -->
                        <tbody>
                            <tr>
                                <td><strong>Options</strong></td>
                                <td style="border-right: 1px"><strong>Checklist</strong></td>
                                <td><strong>Options</strong></td>
                                <td><strong>Checklist</strong></td>
                            </tr>
                            <tr>
                                <td>Beautiful Colors üòä</td>
                                <td style="border-right: 1px">Are the colors beautiful?‚úîÔ∏è</br>Are the colors not ugly?‚úîÔ∏è
                                </td>
                                <td>Good üòä</td>
                                <td>Is the object's movement completely realistic?‚úîÔ∏è</br>Does the object's movement have
                                    no obvious realism issues?‚úîÔ∏è</td>
                            </tr>
                            <tr>
                                <td>Ordinary Colors üòê</td>
                                <td style="border-right: 1px">Are the colors beautiful?‚ùå</br>Are the colors not ugly?‚úîÔ∏è
                                </td>
                                <td>Normal üòê</td>
                                <td>Is the object's movement completely realistic?‚ùå</br>Does the object's movement have
                                    no obvious realism issues?‚úîÔ∏è</td>
                            </tr>
                            <tr>
                                <td>Ugly Colors üò¢</td>
                                <td style="border-right: 1px">Are the colors beautiful?‚ùå</br>Are the colors not ugly?‚ùå
                                </td>
                                <td>Bad üò¢</td>
                                <td>Is the object's movement completely realistic?‚ùå</br>Does the object's movement have
                                    no obvious realism issues?‚ùå</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </details>

            <!-- Âç°Áâá 2 -->
            <details
                style="width: 100%; border: 1px solid #ddd; border-radius: 8px; box-shadow: 0 2px 6px rgba(0,0,0,0.1); overflow: hidden;">
                <summary
                    style="cursor: pointer; padding: 1rem; background-color: #f9f9f9; font-weight: bold; font-size: 1.5rem; border-bottom: 1px solid #ddd;">
                    VisionReward Training
                </summary>
                <div style="padding: 1rem;">
                    <p style="font-size: 1.3em;">VisionReward consists of two components: Checklist & Linear Weighted Summarize</p>
                    <div style="display: flex; flex-direction: row; justify-content: center;">
                        <div style="width: 48%;">
                            </br>
                            <p style="font-size: 1.2em; width: 100%;">Checklist</p>
                            </br>
                            <p style="max-width: 90%;">üî∑ VisionReward is a vision-language generative model to answer a set of judgment questions regarding an image or video with "yes" or "no". We use CogVLM2 as the base model for image understanding, and CogVLM2-Video as the base model for video understanding. We use balanced instruction tuning dataset consisting of judgement questions to fine-tune base VLM.</p>
                        </div>
                        <div style="width: 48%;">
                            </br>
                            <p style="font-size: 1.2em; width: 100%;">Linear Weighted Summarize</p>
                            </br>
                            <p style="max-width: 90%;">üî∑ VisionReward applies a linear weighting to the list of "yes" or "no" responses to obtain the final score. We collect a dataset of human preferences, compute the feature difference for each pair, and then perform regression using the logistic regression model to find the optimal weights that best represent human preferences. </p>
                        </div>
                    </div>
                </div>
            </details>

            <!-- Âç°Áâá 3 -->
            <details
                style="width: 100%; border: 1px solid #ddd; border-radius: 8px; box-shadow: 0 2px 6px rgba(0,0,0,0.1); overflow: hidden;">
                <summary
                    style="cursor: pointer; padding: 1rem; background-color: #f9f9f9; font-weight: bold; font-size: 1.5rem; border-bottom: 1px solid #ddd;">
                    MPO
                </summary>
                <div style="padding: 1rem;">
                    <div style="display: flex; justify-content: center; max-width: 100%;">
                        <p style="max-width: 80%; text-align: left;">üî∑ Although Diffusion-DPO has designed a direct preference optimization method to fine-tune text-to-image diffusion models, it suffers from being biased and over-optimized on certain dimensions such as aesthetic but gets somewhat worse, for example, less safe and more trouble about human body. We reproduce the procedure of Diffusion-DPO and test the fine-tuned model on 10,000 sampled prompts from training data, using VisionReward to score images. The following figures illustrates the changes across various dimensions after applying preference learning using the human preference dataset in which factors are coupled with different trade-offs.</p>
                    </div>
                    <div style="display: flex; justify-content: center; max-width: 100%;">
                        <figure style="max-width: 48%; height: auto;">
                            <image src="https://duanwwww.github.io/VisionReward.github.io/images/compare-pick.jpg"
                                style="width: 90%; height: 400px;">
                            </image>
                            <figcaption>Data analysis.
                            </figcaption> 
                        </figure>
                        <figure style="max-width: 48%; height: auto;">
                            <image src="https://duanwwww.github.io/VisionReward.github.io/images/compare-pick-dpo.jpg"
                                style="width: 90%; height: 400px;">
                            </image>
                            <figcaption>DPO analysis.
                            </figcaption>
                        </figure>
                    </div>
                    <p><strong>We thus propose an algorithm to ensure that all dimensions are reasonably enhanced.</strong></p>
                    <figure style="max-width: 100%; height: auto;">
                        <image src="https://duanwwww.github.io/VisionReward.github.io/images/MPO.png"
                            style="width: 80%; height: auto;">
                        </image>
                        <figcaption>MPO algorithm.
                        </figcaption> 
                    </figure>
                    <div style="display: flex; justify-content: center; max-width: 100%;">
                        <figure style="max-width: 48%; height: auto;">
                            <image src="https://duanwwww.github.io/VisionReward.github.io/images/compare-ours.jpg"
                                style="width: 90%; height: 400px;">
                            </image>
                            <figcaption>Ours data analysis.
                            </figcaption> 
                        </figure>
                        <figure style="max-width: 48%; height: auto;">
                            <image src="https://duanwwww.github.io/VisionReward.github.io/images/compare-ours-dpo.jpg"
                                style="width: 90%; height: 400px;">
                            </image>
                            <figcaption>MPO analysis.
                            </figcaption>
                        </figure>
                    </div>
                </div>
            </details>
        </div>
<!-- 
        <h3>annotation</h3>
        table&questions
        <h3>model</h3>
        scoring examples
        <h3>examples</h3>
        <h3>mpo</h3>
        figure&explanations -->
    </section>

    <!-- <section id="benchmark">
        <h2 style="font-size: 1.5em;">MonetBench: Multi-Dimensional Benchmark</h2>
        <p>To establish a comprehensive evaluation benchmark for both image and video generation, we construct
            Monet-Bench, which contains two specialized test sets: Image-MonetBench and Video-MonetBench. The following
            table shows content and challenge categories for image and video domains. </p>
        <table border="1" cellpadding="5" cellspacing="0"
            style="width: 80%; margin: 20px auto; border-collapse: collapse; font-size: 16px;">
            <thead>
                <tr>
                    <th><strong>Type</strong></th>
                    <th><strong>Image</strong></th>
                    <th><strong>Video</strong></th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Content</strong></td>
                    <td>People, Objects, Animals,Architecture, Landscape,Vehicles, Plants, Food,Others, Scenes</td>
                    <td>Story, Human Activity,Artificial Scene, Others,Natural Animal ActivityPhysical Phenomena</td>
                </tr>
                <tr>
                    <td><strong>Challenge</strong></td>
                    <td>Unreal, Style, History,Fine-grained Detail,Color, Famous Character,Normal, Famous
                        Places,Writing, Complex Combo,Positional, Counting,</td>
                    <td>Material, Angle and Lens,Emotional Expression,Color/Tone, Surreal,World Knowledge,Special
                        Effects, Text,Spatial Relationship,Camera Movement,Logical Consistency,Style, Temporal Speed
                    </td>
                </tr>
            </tbody>
        </table>
        <p><em>Content and Challenge Categories for Image and Video</em></p>
        <div style="display: flex; justify-content: space-between;">
            <div style="display: flex; justify-content: center;width: 48%;height: auto;">
                <figure>
                    <img src="https://duanwwww.github.io/VisionReward.github.io/images/content_type_counts_optimized.png"
                        alt="Content(image)" style="width: 100%; height: auto;">
                </figure>
            </div>
            <div style="display: flex; justify-content: center;width: 48%;height: auto;">
                <figure>
                    <img src="https://duanwwww.github.io/VisionReward.github.io/images/content_video.png"
                        alt="Content(video)" style="width: 100%; height: auto;">
                </figure>
            </div>
        </div>
        <div style="display: flex; justify-content: space-between;">
            <div style="display: flex; justify-content: center;width: 48%;height: auto;">
                <figure>
                    <img src="https://duanwwww.github.io/VisionReward.github.io/images/challenge_type_counts_optimized.png"
                        alt="Challenge(image)" style="width: 100%; height: auto;">
                </figure>
            </div>
            <div style="display: flex; justify-content: center;width: 48%;height: auto;">
                <figure>
                    <img src="https://duanwwww.github.io/VisionReward.github.io/images/challenge_video.png"
                        alt="Challenge(video)" style="width: 100%; height: auto;">
                </figure>
            </div>
        </div>
    </section> -->

    <section id="experiments">
        <h2 style="font-size: 1.5em;">VisionReward Experiments</h2>
        <p> <strong>VisionReward get state-of-the-art results across multiple datasets. Especially in the video aspect, VisionReward outperforms VideoScore by 17.2%.</strong></p>
        <table border="1" cellpadding="10" cellspacing="0"
            style="width: 80%; margin: 20px auto; border-collapse: collapse;font-size: 16px;">
            <thead>
                <tr>
                    <th rowspan="3" style="vertical-align: middle; "><strong>Method</strong></th>
                    <th colspan="3"><strong>Image</strong></th>
                    <th colspan="4"><strong>Video</strong></th>
                </tr>
                <tr>
                    <th rowspan="2" style="vertical-align: middle; "><strong>HPDv2</strong></th>
                    <th colspan="2"><strong>MonetBench</strong></th>
                    <th colspan="2"><strong>GenAI-Bench</strong></th>
                    <th colspan="2"><strong>MonetBench</strong></th>
                </tr>
                <tr>
                    <th><strong>tau*</strong></th>
                    <th><strong>diff**</strong></th>
                    <th><strong>tau</strong></th>
                    <th><strong>diff</strong></th>
                    <th><strong>tau</strong></th>
                    <th><strong>diff</strong></th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td colspan="8" style="font-style: italic; color: #6a1b9a; text-align: left;"><strong>task-specific
                            discriminative models</strong></td>
                </tr>
                <tr>
                    <td>ImageReward</td>
                    <td>74.0</td>
                    <td>48.8</td>
                    <td>56.5</td>
                    <td>48.4</td>
                    <td>72.1</td>
                    <td>55.8</td>
                    <td>58.4</td>
                </tr>
                <tr>
                    <td>PickScore</td>
                    <td>79.8</td>
                    <td>49.8</td>
                    <td>57.6</td>
                    <td><u>52.4</u></td>
                    <td><u>75.4</u></td>
                    <td>57.7</td>
                    <td>61.6</td>
                </tr>
                <tr>
                    <td>HPSv2</td>
                    <td><u>83.3</u></td>
                    <td>48.4</td>
                    <td>55.6</td>
                    <td>49.3</td>
                    <td>73.0</td>
                    <td>59.3</td>
                    <td>62.5</td>
                </tr>
                <tr>
                    <td colspan="8" style="font-style: italic; color: #6a1b9a;text-align: left;"><strong>generative
                            models</strong></td>
                </tr>
                <tr>
                    <td>GPT-4o</td>
                    <td>77.5</td>
                    <td>38.9</td>
                    <td>52.7</td>
                    <td>41.8</td>
                    <td>54.3</td>
                    <td>45.7</td>
                    <td>48.3</td>
                </tr>
                <tr>
                    <td>Gemini</td>
                    <td>60.7</td>
                    <td>27.4</td>
                    <td>55.1</td>
                    <td>46.9</td>
                    <td>61.7</td>
                    <td>52.2</td>
                    <td>56.8</td>
                </tr>
                <tr>
                    <td>VQAScore</td>
                    <td>69.7</td>
                    <td>49.4</td>
                    <td>56.5</td>
                    <td>45.2</td>
                    <td>68.0</td>
                    <td>56.1</td>
                    <td>59.5</td>
                </tr>
                <tr>
                    <td>VideoScore</td>
                    <td>76.8</td>
                    <td>45.8</td>
                    <td>52.5</td>
                    <td>47.8</td>
                    <td>71.4</td>
                    <td>49.1</td>
                    <td>54.9</td>
                </tr>
                <tr>
                    <td><strong>VisionReward (Ours)</strong></td>
                    <td><strong>81.7</strong></td>
                    <td><u><strong>51.8</strong></u></td>
                    <td><u><strong>59.5</strong></u></td>
                    <td><strong>51.8</strong></td>
                    <td><strong>74.4</strong></td>
                    <td><u><strong>64.0</strong></u></td>
                    <td><u><strong>72.1</strong></u></td>
                </tr>
            </tbody>
        </table>
        <p><em>Preference accuracy on multiple dataset. <strong>Bold</strong> denotes the best score within the
                generative models, while <u>underline</u> signifies the best score among all categories. Tau* means
                taking account of ties, and diff** means dropping ties in labels (we drop ties both in labels and
                responses for GPT-4o and Gemini in diff** because too many ties are given by them).</em></p>
        <table border="1" cellpadding="5" cellspacing="0"
            style="width: 80%; margin: 20px auto; border-collapse: collapse; font-size: 16px;">
            <thead>
                <tr>
                    <th rowspan="2" style="vertical-align: middle; "><strong>Method</strong></th>
                    <th colspan="4"><strong>Image</strong></th>
                    <th colspan="4"><strong>Video</strong></th>
                </tr>
                <tr>
                    <th><strong>Composition</strong></th>
                    <th><strong>Quality</strong></th>
                    <th><strong>Fidelity</strong></th>
                    <th><strong>Safety & Emotion</strong></th>
                    <th><strong>Stability</strong></th>
                    <th><strong>Dynamic</strong></th>
                    <th><strong>Physics</strong></th>
                    <th><strong>Preservation</strong></th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>LLaVa<sup>*</sup></td>
                    <td>59.9</td>
                    <td>65.7</td>
                    <td><strong>80.9</strong></td>
                    <td>64.4</td>
                    <td>52.5</td>
                    <td>53.8</td>
                    <td>50.6</td>
                    <td>47.5</td>
                </tr>
                <tr>
                    <td>CogVLM2</td>
                    <td>65.8</td>
                    <td>67.1</td>
                    <td>53.1</td>
                    <td>74.7</td>
                    <td>49.3</td>
                    <td>57.1</td>
                    <td>51.2</td>
                    <td>47.8</td>
                </tr>
                <tr>
                    <td>GPT-4o</td>
                    <td>73.1</td>
                    <td>62.7</td>
                    <td>61.9</td>
                    <td>70.1</td>
                    <td>57.9</td>
                    <td>69.1</td>
                    <td><strong>62.4</strong></td>
                    <td>58.8</td>
                </tr>
                <tr>
                    <td>Gemini</td>
                    <td>69.4</td>
                    <td>59.9</td>
                    <td>59.7</td>
                    <td>74.9</td>
                    <td>58.1</td>
                    <td>71.1</td>
                    <td>58.1</td>
                    <td>59.6</td>
                </tr>
                <tr>
                    <td><strong>VisionReward (Ours)</strong></td>
                    <td><strong>78.8</strong></td>
                    <td><strong>81.1</strong></td>
                    <td><strong>80.9</strong></td>
                    <td><strong>83.9</strong></td>
                    <td><strong>62.8</strong></td>
                    <td><strong>72.6</strong></td>
                    <td>53.1</td>
                    <td><strong>68.0</strong></td>
                </tr>
            </tbody>
        </table>
        <p><em>Accuracy of VisionReward (Ours) and other vision-language models (VLMs) on vision quality questions
                constructed from our annotation.* We test LLaVA-v1.5-7B for image and LLava-Next-Video-34B for
                video.</em></p>
        
        <h2 style="font-size: 1.5em;">MPO Experiments</h2>
        <div style="display: flex; justify-content: center; max-width: 100%;">
            <p style="max-width: 80%; text-align: left;"><strong>We have conducted both automatic and human evaluations. MPO with VisionReward gets leading results across multiple machine metrics and are also most preferred by humans.</strong></p>
        </div>
        <div style="display: flex; justify-content: center; max-width: 100%;">
            <div style="width: 50%;">
                <table style="width: 90%;">
                    <thead>
                      <tr>
                        <th>Methods</th>
                        <th>CLIP</th>
                        <th>Aes</th>
                        <th>HPSv2</th>
                        <th>PickScore</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td>Baseline</td>
                        <td>0.273</td>
                        <td>5.463</td>
                        <td>0.282</td>
                        <td>22.25</td>
                      </tr>
                      <tr>
                        <td>DPO with Pick-a-Pic</td>
                        <td><strong>0.279</strong></td>
                        <td>5.511</td>
                        <td>0.286</td>
                        <td>22.45</td>
                      </tr>
                      <tr>
                        <td>DPO with HPSv2</td>
                        <td>0.277</td>
                        <td>5.599</td>
                        <td><strong>0.292</strong></td>
                        <td>22.58</td>
                      </tr>
                      <tr>
                        <td><strong>MPO (Ours)</strong></td>
                        <td><strong>0.279</strong></td>
                        <td><strong>5.612</strong></td>
                        <td>0.289</td>
                        <td><strong>22.61</strong></td>
                      </tr>
                    </tbody>
                </table>
                <p style="text-align: center;">Evaluation results of multiple metrics on DrawBench. (Image)</p>
            </div>
            <div style="width: 50%;">
                <table style="width: 90%;">
                    <thead>
                      <tr>
                        <th>Methods</th>
                        <th>Composition</th>
                        <th>Quality</th>
                        <th>Fidelity</th>
                        <th>Safety&Emotion</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td>Baseline</td>
                        <td>0.755</td>
                        <td>0.550</td>
                        <td>0.009</td>
                        <td>-0.008</td>
                      </tr>
                      <tr>
                        <td>DPO with Pick-a-Pic</td>
                        <td>0.765</td>
                        <td>0.588</td>
                        <td>0.009</td>
                        <td>-0.009</td>
                      </tr>
                      <tr>
                        <td>DPO with HPSv2</td>
                        <td>0.874</td>
                        <td>0.630</td>
                        <td>0.010</td>
                        <td>-0.004</td>
                      </tr>
                      <tr>
                        <td><strong>MPO (Ours)</strong></td>
                        <td><strong>0.894</strong></td>
                        <td><strong>0.670</strong></td>
                        <td><strong>0.017</strong></td>
                        <td><strong>-0.001</strong></td>
                      </tr>
                    </tbody>
                </table>
                <p style="text-align: center;">Evaluation results analyzed by VisionReward. (Image)</p>
            </div>            
        </div>
        <div style="display: flex; justify-content: center; max-width: 100%;">
            <div style="width: 50%;">
                <table style="width: 90%;">
                    <thead>
                      <tr>
                        <th>Methods</th>
                        <th style="text-align: center;">Human<br>Action</th>
                        <th>Scene</th>
                        <th style="text-align: center;">Multiple<br>Objects</th>
                        <th style="text-align: center;">Appear.<br>Style</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td>Baseline</td>
                        <td>98.20</td>
                        <td>55.60</td>
                        <td>68.43</td>
                        <td><strong>24.20</strong></td>
                      </tr>
                      <tr>
                        <td>VideoScore</td>
                        <td>97.60</td>
                        <td>56.25</td>
                        <td>68.66</td>
                        <td>23.96</td>
                      </tr>
                      <tr>
                        <td>VisionReward</td>
                        <td><strong>98.40</strong></td>
                        <td><strong>57.57</strong></td>
                        <td><strong>71.54</strong></td>
                        <td>24.02</td>
                      </tr>
                    </tbody>
                  </table>
                  <p style="text-align: center;">Evaluation results on VBench. (Video)</p>
            </div>
            <div style="width: 50%;">
                  <table style="width: 90%;">
                    <thead>
                      <tr>
                        <th>Methods</th>
                        <th>Stability</th>
                        <th>Dynamic</th>
                        <th>Physics</th>
                        <th>Preservation</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td>Baseline</td>
                        <td>0.272</td>
                        <td><strong>0.047</strong></td>
                        <td>0.323</td>
                        <td>0.584</td>
                      </tr>
                      <tr>
                        <td>VideoScore</td>
                        <td>0.242</td>
                        <td>0.046</td>
                        <td>0.319</td>
                        <td>0.557</td>
                      </tr>
                      <tr>
                        <td>VisionReward</td>
                        <td><strong>0.309</strong></td>
                        <td>0.036</td>
                        <td><strong>0.337</strong></td>
                        <td><strong>0.661</strong></td>
                      </tr>
                    </tbody>
                  </table>
                  <p style="text-align: center;">Evaluation results on MonetBench. (Video)</p>
            </div>            
        </div>
        <div style="display: flex; justify-content: center; max-width: 100%;">
            <figure style="max-width: 48%; height: auto;">
                <image src="https://duanwwww.github.io/VisionReward.github.io/images/pairwise2.jpg"
                    style="width: 100%; height: 150px;">
                </image>
                <figcaption>Human evaluation of Image MPO.
                </figcaption> 
            </figure>
            <figure style="max-width: 48%; height: auto;">
                <image src="https://duanwwww.github.io/VisionReward.github.io/images/pairwise.jpg"
                    style="width: 100%; height: 150px;">
                </image>
                <figcaption>Human evaluation of Video MPO.
                </figcaption>
            </figure>
        </div>
        <div style="display: flex; gap: 1rem; flex-wrap: wrap;">
            <div style="border: 1px solid #ddd; padding: 1rem; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <div style="display: flex; justify-content: center; max-width: 100%;">
                    <div style="width: 100%;"><p style="width: 100%;">Text Prompt: The massive aircraft carrier battles towering waves, its metallic hull gleaming amid the ocean's furious roar.</p></div>
                    
                    <figure style="max-width: 33%; height: auto;">
                        <video src="https://duanwwww.github.io/VisionReward.github.io/videos/ori_3.mp4"
                            style="width: 90%; height: auto;" controls>
                        </video>
                        <figcaption>original
                        </figcaption>
                    </figure>
                    <figure style="max-width: 33%; height: auto;">
                        <video src="https://duanwwww.github.io/VisionReward.github.io/videos/videoscore_3.mp4"
                            style="width: 90%; height: auto;" controls>
                        </video>
                        <figcaption>DPO with VideoScore
                        </figcaption>
                    </figure>
                    <figure style="max-width: 33%; height: auto;">
                        <video src="https://duanwwww.github.io/VisionReward.github.io/videos/ours_3.mp4"
                            style="width: 90%; height: auto;" controls>
                        </video>
                        <figcaption>MPO with VisionReward(Ours)
                        </figcaption>
                    </figure>
                </div>
            </div>
            <div style="border: 1px solid #ddd; padding: 1rem; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <div style="display: flex; justify-content: center; max-width: 100%;">
                    <div style="width: 100%;"><p style="width: 100%;">A sixteen-year-old boy, lying on his bed in a retro, black-and-white, medium close-up, appears serene and contemplative, with tousled hair and soft shadows.</p></div>
                    
                    <figure style="max-width: 33%; height: auto;">
                        <video src="https://duanwwww.github.io/VisionReward.github.io/videos/ori_1.mp4"
                            style="width: 90%; height: auto;" controls>
                        </video>
                        <figcaption>original
                        </figcaption>
                    </figure>
                    <figure style="max-width: 33%; height: auto;">
                        <video src="https://duanwwww.github.io/VisionReward.github.io/videos/videoscore_1.mp4"
                            style="width: 90%; height: auto;" controls>
                        </video>
                        <figcaption>DPO with VideoScore
                        </figcaption>
                    </figure>
                    <figure style="max-width: 33%; height: auto;">
                        <video src="https://duanwwww.github.io/VisionReward.github.io/videos/ours_1.mp4"
                            style="width: 90%; height: auto;" controls>
                        </video>
                        <figcaption>MPO with VisionReward(Ours)
                        </figcaption>
                    </figure>
                </div>
            </div>
            <div style="border: 1px solid #ddd; padding: 1rem; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <div style="display: flex; justify-content: center; max-width: 100%;">
                    <div style="width: 100%;"><p style="width: 100%;">Amidst rolling hills and olive groves, an elegant young man in fine linen approaches with a serene demeanor and a look of determination and humility.</p></div>
                    
                    <figure style="max-width: 33%; height: auto;">
                        <video src="https://duanwwww.github.io/VisionReward.github.io/videos/ori_0.mp4"
                            style="width: 90%; height: auto;" controls>
                        </video>
                        <figcaption>original
                        </figcaption>
                    </figure>
                    <figure style="max-width: 33%; height: auto;">
                        <video src="https://duanwwww.github.io/VisionReward.github.io/videos/videoscore_0.mp4"
                            style="width: 90%; height: auto;" controls>
                        </video>
                        <figcaption>DPO with VideoScore
                        </figcaption>
                    </figure>
                    <figure style="max-width: 33%; height: auto;">
                        <video src="https://duanwwww.github.io/VisionReward.github.io/videos/ours_0.mp4"
                            style="width: 90%; height: auto;" controls>
                        </video>
                        <figcaption>MPO with VisionReward(Ours)
                        </figcaption>
                    </figure>
                </div>
            </div>
            <div style="border: 1px solid #ddd; padding: 1rem; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <div style="display: flex; justify-content: center; max-width: 100%;">
                    <div style="width: 100%;"><p style="width: 100%;"> A sleek, futuristic spaceship, gleaming with a metallic sheen, has landed softly amidst the historic architecture of Berlin.</p></div>
                    
                    <figure style="max-width: 33%; height: auto;">
                        <video src="https://duanwwww.github.io/VisionReward.github.io/videos/ori_4.mp4"
                            style="width: 90%; height: auto;" controls>
                        </video>
                        <figcaption>original
                        </figcaption>
                    </figure>
                    <figure style="max-width: 33%; height: auto;">
                        <video src="https://duanwwww.github.io/VisionReward.github.io/videos/videoscore_4.mp4"
                            style="width: 90%; height: auto;" controls>
                        </video>
                        <figcaption>DPO with VideoScore
                        </figcaption>
                    </figure>
                    <figure style="max-width: 33%; height: auto;">
                        <video src="https://duanwwww.github.io/VisionReward.github.io/videos/ours_4.mp4"
                            style="width: 90%; height: auto;" controls>
                        </video>
                        <figcaption>MPO with VisionReward(Ours)
                        </figcaption>
                    </figure>
                </div>
            </div>
        </div>
        <!-- <div style="display: flex; justify-content: space-between; flex-wrap: wrap;">
            <div style="display: flex; justify-content: center;width: 48%;height: auto;">
                <figure>
                    <img src="https://duanwwww.github.io/VisionReward.github.io/images/exp_Overall_score.jpg"
                        alt="Overall Score" style="width: 100%; height: auto;">
                    <figcaption>Overall Score</figcaption>
                </figure>
            </div>
            <div style="display: flex; justify-content: center;width: 48%;height: auto;">
                <figure>
                    <img src="https://duanwwww.github.io/VisionReward.github.io/images/exp_Composition_score.jpg"
                        alt="Composition Score" style="width: 100%; height: auto;">
                    <figcaption>Composition Score</figcaption>
                </figure>
            </div>
            <div style="display: flex; justify-content: center;width: 48%;height: auto;">
                <figure>
                    <img src="https://duanwwww.github.io/VisionReward.github.io/images/exp_Fidelity_score.jpg"
                        alt="Fidelity Score" style="width: 100%; height: auto;">
                    <figcaption>Fidelity Score</figcaption>
                </figure>
            </div>
            <div style="display: flex; justify-content: center;width: 48%;height: auto;">
                <figure>
                    <img src="https://duanwwww.github.io/VisionReward.github.io/images/exp_Alignment_score.jpg"
                        alt="Alignment Score" style="width: 100%; height: auto;">
                    <figcaption>Alignment Score</figcaption>
                </figure>
            </div>
            <div style="display: flex; justify-content: center;width: 48%;height: auto;">
                <figure>
                    <img src="https://duanwwww.github.io/VisionReward.github.io/images/exp_Quality_score.jpg"
                        alt="Quality Score" style="width: 100%; height: auto;">
                    <figcaption>Quality Score</figcaption>
                </figure>
            </div>
            <div style="display: flex; justify-content: center;width: 48%;height: auto;">
                <figure>
                    <img src="https://duanwwww.github.io/VisionReward.github.io/images/exp_Safety_and_Emotion_score.jpg"
                        alt="Safety&Emotion Score" style="width: 100%; height: auto;">
                    <figcaption>Safety&Emotion Score</figcaption>
                </figure>
            </div>
        </div> -->
    </section>

    <section id="case_study">
        <h2 style="font-size: 1.5em;">Case Study</h2>
        <div style="display: flex; justify-content: center;">
            <figure style="max-width: 60%; height: auto;">
                <img src="https://duanwwww.github.io/VisionReward.github.io/images/mpo_case_image.jpg"
                    alt="Qualitative result of MPO in text-to-image." style="max-width: 100%; height: auto;" />
                <figcaption>Qualitative result of MPO in text-to-image.</figcaption>
            </figure>
        </div>
        <!-- <div style="display: flex; justify-content: center;">
            <figure style="max-width: 60%; height: auto;">
                <img src="https://duanwwww.github.io/VisionReward.github.io/images/dpo_case_1.jpg"
                    alt="SQualitative result of MPO in text-to-video." style="max-width: 100%; height: auto;" />
                <figcaption>Qualitative result of MPO in text-to-video.</figcaption>
            </figure>
        </div>
        <div style="display: flex; justify-content: center;">
            <figure style="max-width: 60%; height: auto;">
                <img src="https://duanwwww.github.io/VisionReward.github.io/images/dpo_case_2.jpg"
                    alt="Qualitative result of MPO in text-to-video." style="max-width: 100%; height: auto;" />
                <figcaption>Qualitative result of MPO in text-to-video.</figcaption>
            </figure>
        </div> -->
    </section>

    <section id="citation">
        <h2 style="font-size: 1.5em;">Citation</h2>
        <p>VisionReward</p>
    </section>
</body>

</html>